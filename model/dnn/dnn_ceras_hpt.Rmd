---
title: "dnn_ceras_hpt"
author: "Tobias_R"
date: "2 Februar 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(here)
library(Metrics)
```

#Prepare data

# load in data created by add_image_anaylsis_df
```{r}
# load("C:/Users/tobir/OneDrive/Dokumente/R/WS_21_22/DeepL/Berlin-SDL-WS2122/data/image_an_mvars_df.Rda")
load(here("data","metric_pic_abs_final.Rda"))
```

#Choose data set
```{r}
data <- work
#View(data[1:10,])
```


## split dataset into training and test set.
Because k-fold cross-validation is used, there is no need for a validation set

```{r}
## 85% of the sample size
smp_size <- floor(0.85 * nrow(data))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)

train_data <- data[train_ind, ]
test_data <- data[-train_ind, ]
```

```{r}
View(test_data[0:10,])
```


## trnsform price to log price
```{r}
train_targets <- log(train_data["price"]) #vary later to see if val_mae changes
test_targets <- log(test_data["price"]) #vary later to see if val_mae changes

#train_targets <- train_data["price"] #no log
#test_targets <- test_data["price"] # no log

train_data <- subset(x = train_data, select = -c(price))
test_data <- subset(x = test_data, select = -c(price))
```

## transform data to matrix, so that keras can work with it
```{r}
train_targets <- unlist(c(train_targets))
test_targets <- unlist(c(test_targets))
train_data <- array(train_data)
test_data <- array(test_data)
```

## standardize data with z-score
should be the same as sebs
```{r zScores, cache = T}
# parameters for Scaling:
mean <- colMeans(train_data) # mean of each column
std <- apply(train_data, 2, sd) # stdev of each column

# Calculate feature-wise (within-variable) z-scores: (x - mean)/std
train_data <- scale(train_data, center = mean, scale = std)
test_data <- scale(test_data, center = mean, scale = std)
```

```{r}
View(train_data[1:10,])
```





# Hyperparameter tuning

```{r}
library(caret)
library(keras)
```


Tried to install mxnet but an error occurs. 
TO get an approx. for the params, mlpKerasDropout should do
For the dum data set, even for a neuralnet with layers of 500 the std can not be computed. My best guess is, that similiar to the problem with the deep_net script, the informations inside the dummi data set is just not high enough. For dum set with mlpdropout only nans were computed. I guess its the same reason as for the neuralnet. 

The idea behind using neuralnet was to have a analytical way of tuning the architecture of our dnn.

But even for the abs data set, the neuralnet did also not work. 

However the mlpKerasDroput works. So we use that and then set the architecture of our nn our selfes.


```{r}
train_df <- as.data.frame(train_data)

test_df <- as.data.frame(test_data)

fit_control <- trainControl(method = "adaptive_cv", 
                            adaptive = list(min=5, 
                                            alpha=0.05, 
                                            method="BT", 
                                            complete=FALSE),
                            search = "random")

model <- train(train_df, train_targets, 
               method = "mlpKerasDropout", 
               trControl = fit_control,
               preProc = c("center", "scale"),
               verbose = TRUE,
               tuneLength = 20,
               epochs = 100)

preds <- predict(model, test_df)
```

```{r}
results <- model$results
min_rmse_index <- which.min(results$RMSE)
rmse_min <- results[min_rmse_index,]$RMSE
```


```{r}
results[min_rmse_index,]
```


```{r}
#plot 
plot(1:nrow(results), results$RMSE, xlab="Trial", ylab="RMSE", cex=results$size / max(results$size), col="red", ylim=c(1, 30))
text(1:nrow(results), results$RMSE, results$activation, offset=.3, pos=3, cex=0.8)
rect(min_rmse_index-1, rmse_min-1, min_rmse_index+2, rmse_min+2)
```

```{r}
predictions_test <- predict(model, test_df)
rmse(test_targets, predictions_test)

```

