 ---
title: "data_inspection and preperation"
author: "Tobias_R"
date: "28 Januar 2022"
output: html_document
---

#Prepare data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Metrics)
library(keras)
library(here)
```

# load in data 
```{r}
load(here("data","munich", "m_metric_pic_abs_temp.Rda"))
```


```{r}
full_mun <- metric_pic_abs_temp
mun_targets <- log(full_mun["price"])
mun_targets <- unlist(c(mun_targets))
mun_data <- subset(x = full_mun, select = -c(price))
mun_data <- array(mun_data)

mean <- colMeans(mun_data) # mean of each column
std <- apply(mun_data, 2, sd) # stdev of each column
mun_data <- scale(mun_data, center = mean, scale = std)
```

```{r}
load(here("data","berlin", "b_metric_pic_abs_temp.Rda"))
```


```{r}
full_data <- metric_pic_abs_temp
train_data <- train
test_data <- test
```


## split dataset into training and test set.
Because k-fold cross-validation is used, there is no need for a validation set

```{r}
View(test_data[0:10,])
```


## trnsform price to log price
```{r}
train_targets <- log(train_data["price"]) #vary later to see if val_mae changes
test_targets <- log(test_data["price"]) #vary later to see if val_mae changes
full_targets <- log(full_data["price"])

#train_targets <- train_data["price"] #no log
#test_targets <- test_data["price"] # no log

train_data <- subset(x = train_data, select = -c(price))
test_data <- subset(x = test_data, select = -c(price))
full_data <- subset(x = full_data, select = -c(price))

```

## transform data to matrix, so that keras can work with it
```{r}
train_targets <- unlist(c(train_targets))
test_targets <- unlist(c(test_targets))
full_targets <- unlist(c(full_targets))
train_data <- array(train_data)
test_data <- array(test_data)
full_data <- array(full_data)
```


## standardize data with z-score
should be the same as sebs
```{r zScores, cache = T}
# parameters for Scaling:
mean <- colMeans(train_data) # mean of each column
std <- apply(train_data, 2, sd) # stdev of each column

# Calculate feature-wise (within-variable) z-scores: (x - mean)/std
train_data <- scale(train_data, center = mean, scale = std)
test_data <- scale(test_data, center = mean, scale = std)
full_data <- scale(full_data, center = mean, scale = std)
```


```{r}
View(train_data[1:10,])
```


## load in keras etc

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Initialize package
# install.packages(keras)
library(keras)
library(ggplot2)
library(Metrics) #for rmse
```

## Install tensorflow 

It's only necessary to run this once. 

```{r install, eval = F}
# for GPU
# install_keras(tensorflow = "gpu")

# or CPU:
# install_keras() # for cpu
```

# Part 2: Define Network

## This is the base model I started with

```{r}
build_base_model <- function() {
  network <- keras_model_sequential() %>% 
    layer_dense(units = 64, activation = "relu", input_shape = dim(train_data)[2]) %>% 
    layer_dense(units = 64, activation = "relu") %>% 
    layer_dense(units = 1) 
    
  network %>% compile(
    optimizer = "rmsprop", 
    loss = "mse", 
    metrics = c("mae")
  )
}
```


## Model for the abs data set
```{r}
build_abs_model <- function() {
  network <- keras_model_sequential() %>% 
    layer_dense(units = 24, activation = "tanh", input_shape = dim(train_data)[2]) %>%
    layer_dropout(0.17) %>%
    layer_dense(units = 12, activation = "tanh") %>%
    layer_dropout(0.17) %>%
    layer_dense(units = 1) 
    
  network %>% compile(
    optimizer = "rmsprop", 
    loss = "mse", 
    metrics = c("mae")
  )
}
```


## Model in progress for dummi set
Added additional layers and units to account for the higher number of variables in the dum set. 
For a large input layer the leaky relu is suggested. 

- i use sigmoid only in the last one even though caret suggests it in general. 
But if i use leaky_relu i can present it. 
Since it could help with a large data frame, it makes sense here
```{r defModel}
build_dum_model <- function() {
  network <- keras_model_sequential() %>% 
    layer_dense(units = 400, input_shape = dim(train_data)[2]) %>%
    layer_activation_leaky_relu() %>%
    layer_dropout(0.3) %>%
    layer_dense(units = 200) %>%
    layer_activation_leaky_relu() %>%
    layer_dropout(0.3) %>%
    layer_dense(units = 100) %>%
    layer_activation_leaky_relu() %>%
    layer_dropout(0.3) %>%
    layer_dense(units = 50) %>%
    layer_activation_leaky_relu() %>%
    layer_dropout(0.3) %>%
    layer_dense(units = 25) %>%
    layer_activation_leaky_relu() %>%
    layer_dropout(0.3) %>%
    layer_dense(units = 1, activation = "sigmoid") 
    
  network %>% compile(
    optimizer = optimizer_rmsprop(learning_rate  = 0.58, rho = 0.32, decay = 0.41),  
    loss = "mse", 
    metrics = c("mae")
  )
}
```

optimizer_rmsprop(learning_rate  = 0.58, rho = 0.32, decay = 0.41)

Note two new functions here, the mean squared error:

$$\operatorname{MSE} = \frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n} = \frac{\sum_{i=1}^n{e_i^2}}{n}$$
and the mean absolute error (MAE):

$$\mathrm{MAE} = \frac{\sum_{i=1}^n\left| y_i-\hat{y_i}\right|}{n} = \frac{\sum_{i=1}^n\left| e_i\right|}{n}$$
where $\hat{y_i}$ is the predicted value, given in our last single-unit layer, and $y_i$ is the actual value, the label.

# Part 3: k-fold cross validation

```{r setkFold, echo = TRUE, results = 'hide'}
k <- 10 # as suggested by caret
indices <- sample(1:nrow(train_data)) # randomize the training set before splitting for k-fold cross validation:
folds <- cut(indices, breaks = k, labels = FALSE) # divide the ordered indices into k intervals, labelled 1:k.
```


## test if everything works by running cross val with low num_epoch , high batch size, k = 1 and base_model
```{r kfold100, cache = T}
num_epochs <- 10
all_scores <- c() # An empty vector to store the results from evaluation

for (i in 1:1) {
  cat("processing fold #", i, "\n")
  # Prepare the validation data: data from partition # k
  val_indices <- which(folds == i, arr.ind = TRUE) 
  
  # validation set: the ith partition
  val_data <- train_data[val_indices,]
  val_targets <- train_targets[val_indices]
  
  # Training set: all other partitions
  partial_train_data <- train_data[-val_indices,]
  partial_train_targets <- train_targets[-val_indices]
  
  # Call our model function (see above)
  network <- build_base_model() #adjust to eatherbase model or model
  
  # summary(model)
  # Train the model (in silent mode, verbose=0)
  network %>% fit(partial_train_data,
                  partial_train_targets,
                  epochs = num_epochs,
                  batch_size = 250,
                  verbose = 0)
                
  # Evaluate the model on the validation data
  results <- network %>% evaluate(val_data, val_targets, verbose = 0)
  all_scores <- c(all_scores, results[2])
}  
```


We get 4 mae values

```{r allscores}
results
```

### Training with more computational time. Adjust num_epoch and batch size

Let's try training the network for a bit longer: 500 epochs. To keep a record of how well the model did at each epoch, we will modify our training loop to save the per-epoch validation score log:

```{r clearMem}
# Some memory clean-up
K <- backend()
K$clear_session()
```

Train abs_model with suggested hp from caret:

- the batchsize suggested by caret is very high. Try 500 one time
```{r kfold500, echo = T, results = 'hide', cache = T}
num_epochs <- 250
all_mae_histories <- NULL  # an empty object to cumulatively store the model metrics

for (i in 1:k) {
  cat("processing fold #", i, "\n")
  
  # Prepare the validation data: data from partition # k
  val_indices <- which(folds == i, arr.ind = TRUE)
  val_data <- train_data[val_indices,]
  val_targets <- train_targets[val_indices]
  
  # Prepare the training data: data from all other partitions
  partial_train_data <- train_data[-val_indices,]
  partial_train_targets <- train_targets[-val_indices]
  
  # Build the Keras model (already compiled)
  model <- build_abs_model()
  
  # Train the model (in silent mode, verbose=0)
  history <- model %>% fit(partial_train_data, 
                           partial_train_targets,
                           validation_data = list(val_data, val_targets),
                           epochs = num_epochs, 
                           batch_size = 100, 
                           verbose = 0
  )
  mae_history <- history$metrics$val_mae
  all_mae_histories <- rbind(all_mae_histories, mae_history)
}
```

Calculate the average per-epoch MAE score for all folds and print graph:

```{r plot1}
average_mae_history <- data.frame(
  epoch = seq(1:ncol(all_mae_histories)),
  validation_mae = apply(all_mae_histories, 2, mean)
)

p <- ggplot(average_mae_history, aes(x = epoch, y = validation_mae))

p + 
  geom_point(size = 0.01)

p + 
  geom_smooth(e = FALSE)
```
#Find the lowest value
```{r}
which(x = average_mae_history$validation_mae == min(average_mae_history))
average_mae_history
```


Train model one last time with hyperparameters as defined above

```{r runZ, echo = F, results = 'hide', cache = T}
# Get a fresh, compiled model.
model <- build_abs_model()

# Train it on the entirety of the data.
model %>% fit(full_data, 
              full_targets,
              epochs = 160, 
              batch_size = 100, 
              verbose = 1)

result <- model %>% evaluate(test_data, test_targets)
```

```{r resultsZ}
result
```

```{r}
#model %>% save_model_tf("nn_abs_full")
```

```{r}
model <- load_model_tf("nn_abs_full")
```


# Predict and calc rmse
```{r}
predictions_test <- model %>% predict(x = test_data)
predictions_train <- model %>% predict(x = train_data)
```


```{r}
rmse(test_targets, predictions_test)
rmse(train_targets, predictions_train)
```

```{r}
predictions_mun <- model %>% predict(x = mun_data)
rmse(mun_targets, predictions_mun)
```


