---
title: "data_inspection and preperation"
author: "Tobias_R"
date: "28 Januar 2022"
output: html_document
---

#Prepare data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
load("data/image_an_mvars_df.Rda")
```

```{r}
image_an_mvars_df <- image_an_mvars_df[,3:length(image_an_mvars_df)]
```

## leave out longitude and latitude
```{r}
image_an_mvars_df <- subset(x = image_an_mvars_df, select = -c(latitude, longitude))
#View(dataset_mod)
```

```{r}
for (i in 1:dim(image_an_mvars_df)[2]){
  image_an_mvars_df[,i] <-  unlist(image_an_mvars_df[,i])
}
```

# check if every column is not a list
```{r}
truth_list <- list()
for (i in 1:dim(image_an_mvars_df)[2]){
  truth_list[i] <-  is.list(image_an_mvars_df[,i])
}

sum(unlist(truth_list))
```

```{r}
truth_num <- list()
for (i in 1:dim(image_an_mvars_df)[2]){
  truth_num[i] <-  which(is.numeric(image_an_mvars_df[,i]))
}

sum(unlist(truth_num)) 
dim(image_an_mvars_df)[2]
```


```{r}
View(image_an_mvars_df)
```


Find out what causes nans to come up

reduce columns
```{r}
image_an_mvars_df <- image_an_mvars_df[,1:60]
```

```{r}
View(image_an_mvars_df)
```



## split dataset into training and test set.
Because k-fold cross-validation is used, there is no need for a validation set
```{r}
## 75% of the sample size
smp_size <- floor(0.75 * nrow(image_an_mvars_df))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(image_an_mvars_df)), size = smp_size)

train_data <- image_an_mvars_df[train_ind, ]
test_data <- image_an_mvars_df[-train_ind, ]
```

```{r}
View(test_data[0:10,])
```


## trnsform price to log price
```{r}
train_targets <- log(train_data["price"]) #vary later to see if val_mae changes
test_targets <- log(test_data["price"]) #vary later to see if val_mae changes

#train_targets <- train_data["price"] #no log
#test_targets <- test_data["price"] # no log

train_data <- subset(x = train_data, select = -c(price))
test_data <- subset(x = test_data, select = -c(price))
```

```{r}
View(test_data[0:10,])
```


## transform data to matrix, so that keras can work with it
```{r}
train_targets <- unlist(c(train_targets))
test_targets <- unlist(c(test_targets))
train_data <- array(train_data)
test_data <- array(test_data)
```

```{r}
head(train_data)
```



```{r}
#View(train_targets)
```

## standardize data with z-score
should be the same as sebs
```{r zScores, cache = T}
# parameters for Scaling:
mean <- colMeans(train_data) # mean of each column
std <- apply(train_data, 2, sd) # stdev of each column

# Calculate feature-wise (within-variable) z-scores: (x - mean)/std
train_data <- scale(train_data, center = mean, scale = std)
test_data <- scale(test_data, center = mean, scale = std)
```

```{r}
View(train_data[1:10,])
```


## load in keras etc

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Initialize package
# install.packages(keras)
library(keras)
library(ggplot2)
library(Metrics) #for rmse
```

## Install tensorflow 

It's only necessary to run this once. 

```{r install, eval = F}
# for GPU
# install_keras(tensorflow = "gpu")

# or CPU:
# install_keras() # for cpu
```

# Part 2: Define Network

## Define the network as a function

We're going to call the same model multiple times. So we'll create a function with no arguments that we can call to create our model when ever we want to use it for training. 

Added one additional layer to account for the higher number of variables. Does that make sense?

```{r defModel}
build_model <- function() {
  network <- keras_model_sequential() %>% 
    layer_dense(units = 64, activation = "relu", input_shape = dim(train_data)[2]) %>% 
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 1) 
    
  network %>% compile(
    optimizer = "rmsprop", 
    loss = "mse", 
    metrics = c("mae")
  )
}
```

Note two new functions here, the mean squared error:

$$\operatorname{MSE} = \frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n} = \frac{\sum_{i=1}^n{e_i^2}}{n}$$
and the mean absolute error (MAE):

$$\mathrm{MAE} = \frac{\sum_{i=1}^n\left| y_i-\hat{y_i}\right|}{n} = \frac{\sum_{i=1}^n\left| e_i\right|}{n}$$
where $\hat{y_i}$ is the predicted value, given in our last single-unit layer, and $y_i$ is the actual value, the label.

# Part 3: k-fold cross validation

```{r setkFold, echo = TRUE, results = 'hide'}
k <- 4 # four groups
indices <- sample(1:nrow(train_data)) # randomize the training set before splitting for k-fold cross validation:
folds <- cut(indices, breaks = k, labels = FALSE) # divide the ordered indices into k intervals, labelled 1:k.
```

#test loop there are nans in the datasets ... find out where they come from

```{r}
num_epochs <- 100
all_scores <- c() # An empty vector to store the results from evaluation

i = 1
cat("processing fold #", i, "\n")
# Prepare the validation data: data from partition # k
val_indices <- which(folds == i, arr.ind = TRUE) 

# validation set: the ith partition
val_data <- train_data[val_indices,]
val_targets <- train_targets[val_indices]

# Training set: all other partitions
partial_train_data <- train_data[-val_indices,]
partial_train_targets <- train_targets[-val_indices]

# Call our model function (see above)
network <- build_model()

# summary(model)
# Train the model (in silent mode, verbose=0)
network %>% fit(partial_train_data,
                partial_train_targets,
                epochs = num_epochs,
                batch_size = 100,
                verbose = 0)
              
# Evaluate the model on the validation data
results <- network %>% evaluate(val_data, val_targets, verbose = 0)
all_scores <- c(all_scores, results[2])
```

```{r}
View(partial_train_data)
```




## test if everything works by running cross val with low num_epoch and high batch size

```{r kfold100, cache = T}
num_epochs <- 100
all_scores <- c() # An empty vector to store the results from evaluation

for (i in 1:k) {
  cat("processing fold #", i, "\n")
  # Prepare the validation data: data from partition # k
  val_indices <- which(folds == i, arr.ind = TRUE) 
  
  # validation set: the ith partition
  val_data <- train_data[val_indices,]
  val_targets <- train_targets[val_indices]
  
  # Training set: all other partitions
  partial_train_data <- train_data[-val_indices,]
  partial_train_targets <- train_targets[-val_indices]
  
  # Call our model function (see above)
  network <- build_model()
  
  # summary(model)
  # Train the model (in silent mode, verbose=0)
  network %>% fit(partial_train_data,
                  partial_train_targets,
                  epochs = num_epochs,
                  batch_size = 100,
                  verbose = 0)
                
  # Evaluate the model on the validation data
  results <- network %>% evaluate(val_data, val_targets, verbose = 0)
  all_scores <- c(all_scores, results[2])
}  
```


We get 4 mae values

```{r allscores}
results
```

### Training with more computational time. Adjust num_epoch and batch size

Let's try training the network for a bit longer: 500 epochs. To keep a record of how well the model did at each epoch, we will modify our training loop to save the per-epoch validation score log:

```{r clearMem}
# Some memory clean-up
K <- backend()
K$clear_session()
```

Train our the models:

```{r kfold500, echo = T, results = 'hide', cache = T}
num_epochs <- 100 
all_mae_histories <- NULL  # an empty object to cumulatively store the model metrics

for (i in 1:k) {
  cat("processing fold #", i, "\n")
  
  # Prepare the validation data: data from partition # k
  val_indices <- which(folds == i, arr.ind = TRUE)
  val_data <- train_data[val_indices,]
  val_targets <- train_targets[val_indices]
  
  # Prepare the training data: data from all other partitions
  partial_train_data <- train_data[-val_indices,]
  partial_train_targets <- train_targets[-val_indices]
  
  # Build the Keras model (already compiled)
  model <- build_model()
  
  # Train the model (in silent mode, verbose=0)
  history <- model %>% fit(partial_train_data, 
                           partial_train_targets,
                           validation_data = list(val_data, val_targets),
                           epochs = num_epochs, 
                           batch_size = 500, 
                           verbose = 0
  )
  mae_history <- history$metrics$val_mae
  all_mae_histories <- rbind(all_mae_histories, mae_history)
}
```

Calculate the average per-epoch MAE score for all folds and print graph:

```{r plot1}
average_mae_history <- data.frame(
  epoch = seq(1:ncol(all_mae_histories)),
  validation_mae = apply(all_mae_histories, 2, mean)
)

p <- ggplot(average_mae_history, aes(x = epoch, y = validation_mae))

p + 
  geom_point()

p + 
  geom_smooth(method = 'loess', se = FALSE)
```
Find the lowest value
```{r}
which(x = average_mae_history$validation_mae == min(average_mae_history))
average_mae_history
```


According to this plot, it seems that validation MAE stops improving significantly after circa i epochs. Past that point, we start overfitting.

Once we are done tuning other parameters of our model (besides the number of epochs, we could also adjust the size of the hidden layers), we can train a final "production" model on all of the training data, with the best parameters, then look at its performance on the test data:

```{r runZ, echo = F, results = 'hide', cache = T}
# Get a fresh, compiled model.
model <- build_model()

# Train it on the entirety of the data.
model %>% fit(train_data, 
              train_targets,
              epochs = 26, 
              batch_size = 50, 
              verbose = 0)

result <- model %>% evaluate(test_data, test_targets)
```

```{r resultsZ}
result
```

We are still off by about `r round(result$mean_absolute_error * 1000)`.

```{r}
predictions_test <- model %>% predict(x = test_data)
predictions_train <- model %>% predict(x = train_data)
```


```{r}
rmse(test_targets, predictions_test)
rmse(train_targets, predictions_train)
```

