% This file was created with Citavi 6.11.0.0

@book{.2013,
 year = {2013},
 title = {Empirical Inference},
 publisher = {{Springer, Berlin, Heidelberg}}
}


@misc{He.20.03.2017,
 abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
 author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
 date = {20.03.2017},
 title = {Mask R-CNN},
 url = {https://arxiv.org/pdf/1703.06870}
}


@article{Hofner.2014,
 author = {Hofner, Benjamin and Mayr, Andreas and Robinzonov, Nikolay and Schmid, Matthias},
 year = {2014},
 title = {Model-based Boosting in R: A Hands-on Tutorial Using the  R Package mboost},
 pages = {3--35},
 volume = {29},
 journal = {Computational Statistics},
 file = {mboost:Attachments/mboost.pdf:application/pdf}
}


@misc{InsideAirbnb.2022,
 abstract = {Inside Airbnb is an independent, non-commercial set of tools and data that allows you to explore how Airbnb is REALLY being used in cities around the world.},
 author = {{Inside Airbnb}},
 year = {2022},
 title = {Inside Airbnb. Adding data to the debate},
 url = {http://insideairbnb.com/about.html},
 urldate = {01.03.2022}
}


@book{James.2021,
 abstract = {Preface -- 1 Introduction -- 2 Statistical Learning -- 3 Linear Regression -- 4 Classification -- 5 Resampling Methods -- 6 Linear Model Selection and Regularization -- 7 Moving Beyond Linearity -- 8 Tree-Based Methods -- 9 Support Vector Machines -- 10 Deep Learning -- 11 Survival Analysis and Censored Data -- 12 Unsupervised Learning -- 13 Multiple Testing -- Index.



An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, deep learning, survival analysis, multiple testing, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra. This Second Edition features new chapters on deep learning, survival analysis, and multiple testing, as well as expanded treatments of na{\"i}ve Bayes, generalized linear models, Bayesian additive regression trees, and matrix completion. R code has been updated throughout to ensure compatibility.},
 author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
 year = {2021},
 title = {An Introduction to Statistical Learning: With Applications in R},
 address = {New York, NY},
 edition = {2nd ed. 2021},
 publisher = {{Springer US} and {Imprint: Springer}},
 isbn = {9781071614181},
 series = {Springer eBook Collection},
 doi = {10.1007/978-1-0716-1418-1},
 file = {ISLRv2{\_}website:Attachments/ISLRv2{\_}website.pdf:application/pdf}
}


@article{Kalehbasti.2021,
 abstract = {Pricing a rental property on Airbnb is a challenging task for the owner as it determines the number of customers for the place. On the other hand, customers have to evaluate an offered price with minimal knowledge of an optimal value for the property. This paper aims to develop a reliable price prediction model using machine learning, deep learning, and natural language processing techniques to aid both the property owners and the customers with price evaluation given minimal available information about the property. Features of the rentals, owner characteristics, and the customer reviews will comprise the predictors, and a range of methods from linear regression to tree-based models, support-vector regression (SVR), K-means Clustering (KMC), and neural networks (NNs) will be used for creating the prediction model.},
 author = {Kalehbasti, Pouya Rezazadeh and Nikolenko, Liubov and Rezaei, Hoormazd},
 year = {2021},
 title = {Airbnb Price Prediction Using Machine Learning and Sentiment Analysis},
 pages = {173--184},
 volume = {12844},
 number = {3},
 journal = {International Cross-Domain Conference for Machine Learning and Knowledge Extraction, pp. 173-184. Springer, Cham},
 file = {Kalehbasti, Nikolenko et al. 2021 - Airbnb Price Prediction Using Machine (2):Attachments/Kalehbasti, Nikolenko et al. 2021 - Airbnb Price Prediction Using Machine (2).pdf:application/pdf}
}


@book{Leibe.2016,
 year = {2016},
 title = {Computer vision - ECCV 2016: 14th European conference, Amsterdam, The Netherlands, October 11-14, 2016 : proceedings},
 address = {Cham},
 volume = {9905},
 publisher = {Springer},
 isbn = {978-3-319-46447-3},
 series = {Lecture Notes in Computer Science},
 editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
 doi = {10.1007/978-3-319-46448-0}
}


@book{Lewis.February2016,
 author = {Lewis, N. D.},
 year = {February 2016},
 title = {Deep learning made easy with R : a gentle introduction for data science},
 address = {Place of publication not identified},
 publisher = {AusCov},
 isbn = {9781519514219},
 file = {N.D Lewis - Deep Learning Made Easy with R{\_} A Gentle Introduction For Data Science-CreateSpace Independent Publishing Platform (2016):Attachments/N.D Lewis - Deep Learning Made Easy with R{\_} A Gentle Introduction For Data Science-CreateSpace Independent Publishing Platform (2016).pdf:application/pdf}
}


@misc{Lin.07.08.2017,
 abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
 author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
 date = {07.08.2017},
 title = {Focal Loss for Dense Object Detection},
 url = {https://arxiv.org/pdf/1708.02002}
}


@incollection{Liu.2016,
 author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
 title = {SSD: Single Shot MultiBox Detector},
 pages = {21--37},
 volume = {9905},
 publisher = {Springer},
 isbn = {978-3-319-46447-3},
 series = {Lecture Notes in Computer Science},
 editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
 booktitle = {Computer vision - ECCV 2016},
 year = {2016},
 address = {Cham},
 doi = {10.1007/978-3-319-46448-0{\textunderscore }2}
}


@misc{MachineCurve.2019,
 abstract = {Leaky ReLU can improve your traditional ReLU by resolving some problems of the latter. Learn how and why it works. Explanations and examples.},
 author = {MachineCurve},
 year = {2019},
 title = {Leaky ReLU: improving traditional ReLU -- MachineCurve},
 url = {https://www.machinecurve.com/index.php/2019/10/15/leaky-relu-improving-traditional-relu/},
 urldate = {02.02.2022}
}


@misc{NikhithTayambhath.2017,
 abstract = {I tried this code:

import cv2

image = cv2.imread({\textquotedbl}sample.jpg{\textquotedbl})

pixel = image[200, 550]

print pixel

But I am getting error as:

'Nonetype' no attributes error getitem

This error is getting displayed after executing the third line of code.},
 author = {{Nikhith Tayambhath}},
 year = {2017},
 title = {How to find the average colour of an image in Python with OpenCV?},
 url = {https://stackoverflow.com/questions/43111029/how-to-find-the-average-colour-of-an-image-in-python-with-opencv},
 urldate = {01.02.2022}
}


@misc{OpenGenusIQ:ComputingExpertise&Legacy.2020,
 abstract = {Beschreibung von Resnet50, default model in unserem object detector},
 author = {{OpenGenus IQ: Computing Expertise {\&} Legacy}},
 year = {2020},
 title = {Understanding ResNet50 architecture},
 url = {https://iq.opengenus.org/resnet50-architecture/},
 urldate = {16.01.2022}
}


@article{Poursaeed.2018,
 author = {Poursaeed, Omid and Matera, Tom{\'a}{\v{s}} and Belongie, Serge},
 year = {2018},
 title = {Vision-based real estate price estimation},
 pages = {667--676},
 volume = {29},
 number = {4},
 issn = {0932-8092},
 journal = {Machine Vision and Applications},
 doi = {10.1007/s00138-018-0922-2}
}


@misc{Redmon.08.06.2015,
 abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
 author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
 date = {08.06.2015},
 title = {You Only Look Once: Unified, Real-Time Object Detection},
 url = {https://arxiv.org/pdf/1506.02640},
 file = {1506.02640:Attachments/1506.02640.pdf:application/pdf}
}


@misc{ResearchGate.15.02.2022,
 abstract = {Download scientific diagram | The architecture of ResNet-50 model. from publication: Performance Evaluation of Deep CNN-Based Crack Detection and Localization Techniques for Concrete Structures | This paper proposes a customized convolutional neural network for crack detection in concrete structures. The proposed method is compared to four existing deep learning methods based on training data size, data heterogeneity, network complexity, and the number of epochs. The... | Concrete Structures, Performance Evaluation and Localization | ResearchGate, the professional network for scientists.},
 author = {ResearchGate},
 year = {15.02.2022},
 title = {Figure 6. The architecture of ResNet-50 model},
 url = {https://www.researchgate.net/figure/The-architecture-of-ResNet-50-model_fig4_349717475},
 urldate = {15.02.2022}
}


@incollection{Schapire.2013,
 abstract = {BoostingBoosting---( is an approach to machine learning based on the idea of creating a highly accurate prediction rule by combining many relatively weak and inaccurate rules. The AdaBoost algorithm of...},
 author = {Schapire, Robert E.},
 title = {Explaining AdaBoost},
 url = {https://link.springer.com/chapter/10.1007/978-3-642-41136-6_5},
 pages = {37--52},
 publisher = {{Springer, Berlin, Heidelberg}},
 booktitle = {Empirical Inference},
 year = {2013},
 doi = {10.1007/978-3-642-41136-6{\textunderscore }5}
}


@article{Simon.2011,
 abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of $\ell$1 and $\ell$2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
 author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
 year = {2011},
 title = {Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent},
 pages = {1--13},
 volume = {39},
 number = {5},
 issn = {1548-7660},
 journal = {Journal of statistical software},
 doi = {10.18637/jss.v039.i05},
 file = {Simon, Friedman et al. 2011 - Regularization Paths for Cox's Proportional:Attachments/Simon, Friedman et al. 2011 - Regularization Paths for Cox's Proportional.pdf:application/pdf}
}


@book{Hastie.2009,
 author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
 year = {2009},
 title = {The elements of statistical learning: Data mining, inference, and prediction /   Trevor Hastie, Robert Tibshirani, Jerome Friedman},
 price = {{\pounds}55.99},
 address = {New York},
 edition = {2nd ed.},
 publisher = {Springer},
 isbn = {9780387848570},
 series = {Springer series in statistics},
 file = {ESLII:Attachments/ESLII.pdf:application/pdf}
}


@inproceedings{Haldar.2019,
 author = {Haldar, Malay and Abdool, Mustafa and Ramanathan, Prashant and Xu, Tao and Yang, Shulin and Duan, Huizhong and Zhang, Qing and Barrow-Williams, Nick and Turnbull, Bradley C. and Collins, Brendan M. and Legrand, Thomas},
 title = {Applying Deep Learning to Airbnb Search},
 pages = {1927--1935},
 publisher = {{Association for Computing Machinery}},
 isbn = {9781450362016},
 series = {ACM Digital Library},
 editor = {Teredesai, Ankur},
 booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery {\&} Data Mining},
 year = {2019},
 address = {New York,NY,United States},
 doi = {10.1145/3292500.3330658},
 file = {Haldar, Abdool et al. 07252019 - Applying Deep Learning to Airbnb:Attachments/Haldar, Abdool et al. 07252019 - Applying Deep Learning to Airbnb.pdf:application/pdf}
}


@misc{GitHub.16.01.2022,
 abstract = {A one-stop repository for low-code easily-installable object detection pipelines. - Tessellate-Imaging/Monk{\_}Object{\_}Detection:},
 author = {GitHub},
 year = {16.01.2022},
 title = {Tessellate-Imaging/Monk{\_}Object{\_}Detection: A one-stop repository for low-code easily-installable object detection pipelines},
 url = {https://github.com/Tessellate-Imaging/Monk_Object_Detection},
 urldate = {16.01.2022}
}


@misc{GitHub.16.01.2022b,
 abstract = {A one-stop repository for low-code easily-installable object detection pipelines. - Monk{\_}Object{\_}Detection/Example - Indoor Image Object Detection and Tagging.ipynb at master · Tessellate-Imaging/Monk{\_}Object{\_}Detection},
 author = {GitHub},
 year = {16.01.2022},
 title = {Monk{\_}Object{\_}Detection/Example - Indoor Image Object Detection and Tagging.ipynb at master · Tessellate-Imaging/Monk{\_}Object{\_}Detection},
 url = {https://github.com/Tessellate-Imaging/Monk_Object_Detection/blob/master/application_model_zoo/Example%20-%20Indoor%20Image%20Object%20Detection%20and%20Tagging.ipynb},
 urldate = {16.01.2022}
}


@misc{.01.11.2019,
 year = {01.11.2019},
 title = {HSP Color Model - Alternative to HSV (HSB) and HSL},
 url = {http://alienryderflex.com/hsp.html},
 urldate = {19.02.2022}
}


@misc{.29.01.2021,
 year = {29.01.2021},
 title = {Colour Temperature --- Colour 0.3.15 documentation},
 url = {https://colour.readthedocs.io/en/v0.3.15/colour.temperature.html},
 urldate = {18.01.2022}
}


@misc{.04.10.2021,
 abstract = {Selenium um {\"u}ber Websiten zu crawlen und bilder herunter zu laden},
 year = {04.10.2021},
 title = {Selenium with Python --- Selenium Python Bindings 2 documentation},
 url = {https://selenium-python.readthedocs.io/},
 urldate = {16.01.2022}
}


@misc{.16.01.2022,
 year = {16.01.2022},
 title = {Effect of batch size on training dynamics | by Kevin Shen | Mini Distill | Medium},
 url = {https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e},
 urldate = {16.01.2022}
}


@misc{.16.01.2022b,
 abstract = {Zusammenfassung von Monk AI funktionen},
 year = {16.01.2022},
 title = {Monk{\_}object{\_}detection},
 url = {https://awesomeopensource.com/project/Tessellate-Imaging/Monk_Object_Detection},
 urldate = {16.01.2022}
}


@misc{.16.01.2022c,
 abstract = {Beschreibung f{\"u}r ein Monk Obect{\_}Detection Projekt als Orientierung},
 year = {16.01.2022},
 title = {Object Detection made easy with Monk AI. | by Het Shah | The Startup | Medium},
 url = {https://medium.com/swlh/object-detection-made-easy-with-monk-ai-e994a0497b33},
 urldate = {16.01.2022}
}


@misc{.01.02.2022,
 year = {01.02.2022},
 title = {Jupyter Notebook Viewer},
 url = {https://nbviewer.org/github/colour-science/colour-notebooks/blob/master/notebooks/temperature/cct.ipynb},
 urldate = {01.02.2022}
}


@misc{Airbnb.02.03.2022,
 abstract = {Entdecke, was neu auf Airbnb ist.},
 author = {Airbnb},
 year = {02.03.2022},
 title = {Airbnb: einzigartige Unterk{\"u}nfte und Aktivit{\"a}ten},
 url = {https://www.airbnb.ch/},
 urldate = {02.03.2022}
}


@proceedings{Teredesai.2019,
 year = {2019},
 title = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery {\&} Data Mining},
 address = {New York,NY,United States},
 publisher = {{Association for Computing Machinery}},
 isbn = {9781450362016},
 series = {ACM Digital Library},
 editor = {Teredesai, Ankur},
 institution = {{ACM Special Interest Group on Management of Data} and {ACM Special Interest Group on Knowledge Discovery in Data}},
 doi = {10.1145/3292500}
}


@article{alAli.2021,
 abstract = {This paper proposes a customized convolutional neural network for crack detection in concrete structures. The proposed method is compared to four existing deep learning methods based on training data size, data heterogeneity, network complexity, and the number of epochs. The performance of the proposed convolutional neural network (CNN) model is evaluated and compared to pretrained networks, i.e., the VGG-16, VGG-19, ResNet-50, and Inception V3 models, on eight datasets of different sizes, created from two public datasets. For each model, the evaluation considered computational time, crack localization results, and classification measures, e.g., accuracy, precision, recall, and F1-score. Experimental results demonstrated that training data size and heterogeneity among data samples significantly affect model performance. All models demonstrated promising performance on a limited number of diverse training data; however, increasing the training data size and reducing diversity reduced generalization performance, and led to overfitting. The proposed customized CNN and VGG-16 models outperformed the other methods in terms of classification, localization, and computational time on a small amount of data, and the results indicate that these two models demonstrate superior crack detection and localization for concrete structures.},
 author = {alAli, Luqman and Alnajjar, Fady and Jassmi, Hamad Al and Gocho, Munkhjargal and Khan, Wasif and Serhani, M. Adel},
 year = {2021},
 title = {Performance Evaluation of Deep CNN-Based Crack Detection and Localization Techniques for Concrete Structures},
 volume = {21},
 number = {5},
 journal = {Sensors (Basel, Switzerland)},
 doi = {10.3390/s21051688},
 file = {Ali, Alnajjar et al. 2021 - Performance Evaluation of Deep CNN-Based:Attachments/Ali, Alnajjar et al. 2021 - Performance Evaluation of Deep CNN-Based.pdf:application/pdf}
}


@book{Breiman.2017,
 abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
 author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
 year = {2017},
 title = {Classification And Regression Trees},
 url = {https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman-jerome-friedman-richard-olshen-charles-stone},
 publisher = {Routledge},
 isbn = {9781315139470},
 doi = {10.1201/9781315139470},
 file = {Breiman, Friedman et al. 2017 - Classification And Regression Trees:Attachments/Breiman, Friedman et al. 2017 - Classification And Regression Trees.pdf:application/pdf}
}


@article{Bushaev.02.09.2018,
 abstract = {Disclaimer: I presume basic knowledge about neural network optimization algorithms. Particularly, knowledge about SGD and SGD with momentum will be very helpful to understand this post. RMSprop--- is$\ldots$},
 author = {Bushaev, Vitaly},
 year = {02.09.2018},
 title = {Understanding RMSprop --- faster neural network learning},
 url = {https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a},
 urldate = {02.02.2022},
 journal = {Towards Data Science},
 file = {Bushaev 02.09.2018 - Understanding RMSprop - faster neural:Attachments/Bushaev 02.09.2018 - Understanding RMSprop - faster neural.pdf:application/pdf}
}


@book{Chollet.2018,
 author = {Chollet, Fran{\c{c}}ois and Allaire, J. J.},
 year = {2018},
 title = {Deep learning with R},
 address = {Shelter Island},
 publisher = {{Manning Publications}},
 isbn = {9781617295546},
 file = {Fran{\c{c}}ois Chollet, J.J. Allaire - Deep Learning with R (2017, Manning Publications) - libgen.lc:Attachments/Fran{\c{c}}ois Chollet, J.J. Allaire - Deep Learning with R (2017, Manning Publications) - libgen.lc.pdf:application/pdf}
}


@misc{DevelopPaper.2020,
 abstract = {Provide a platform for developers to answer questions, learn and exchange programming knowledge, and create an era that belongs to developers!},
 author = {{Develop Paper}},
 year = {2020},
 title = {Activation function of attention mechanism: adaptive parameterized relu activation function - Develop Paper},
 url = {https://developpaper.com/activation-function-of-attention-mechanism-adaptive-parameterized-relu-activation-function/},
 urldate = {19.02.2022}
}


@misc{Dobovizki.18.01.2022,
 author = {Dobovizki, Nir},
 year = {18.01.2022},
 title = {Calculating the Perceived Brightness of a Color},
 url = {https://www.nbdtech.com/Blog/archive/2008/04/27/Calculating-the-Perceived-Brightness-of-a-Color.aspx},
 urldate = {18.01.2022}
}


@book{Efron.1993,
 author = {Efron, Bradley and Tibshirani, Robert J.},
 year = {1993},
 title = {An introduction to the bootstrap},
 address = {New York},
 publisher = {{Chapman {\&} Hall}},
 isbn = {978-0-412-04231-7},
 doi = {10.1007/978-1-4899-4541-9},
 file = {Efron, Tibshirani 1993 - An introduction to the bootstrap:Attachments/Efron, Tibshirani 1993 - An introduction to the bootstrap.pdf:application/pdf}
}


@article{Friedman.2010,
 abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include $\ell$(1) (the lasso), $\ell$(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
 author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
 year = {2010},
 title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
 pages = {1--22},
 volume = {33},
 number = {1},
 issn = {1548-7660},
 journal = {Journal of statistical software}
}


@misc{Girshick.11.11.2013,
 abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
 author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
 date = {11.11.2013},
 title = {Rich feature hierarchies for accurate object detection and semantic  segmentation},
 url = {https://arxiv.org/pdf/1311.2524},
 file = {Girshick, Donahue et al. 11.11.2013 - Rich feature hierarchies for accurate:Attachments/Girshick, Donahue et al. 11.11.2013 - Rich feature hierarchies for accurate.pdf:application/pdf}
}


@article{Breiman.2001,
 abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
 author = {Breiman, Leo},
 year = {2001},
 title = {Random Forests},
 url = {https://link.springer.com/article/10.1023/a:1010933404324},
 pages = {5--32},
 volume = {45},
 number = {1},
 issn = {1573-0565},
 journal = {Machine Learning},
 doi = {10.1023/A:1010933404324},
 file = {Breiman 2001 - Random Forests:Attachments/Breiman 2001 - Random Forests.pdf:application/pdf}
}


@article{Xu.2018,
 abstract = {Model validation is the most important part of building a supervised model. For building a~model with good generalization performance one must have a sensible data splitting strategy, and this is crucial for model validation. In this study, we conducted a comparative study on various reported data splitting methods. The MixSim model was employed to generate nine simulated datasets with different probabilities of mis-classification and variable sample sizes. Then partial least squares for discriminant analysis and support vector machines for classification were applied to these datasets. Data splitting methods tested included variants of cross-validation, bootstrapping, bootstrapped Latin partition, Kennard-Stone algorithm (K-S) and sample~set partitioning based on joint X-Y distances algorithm (SPXY). These methods were employed to split the data into training and validation sets. The estimated generalization performances from the validation sets were then compared with the ones obtained from the blind test sets which were generated from the same distribution but were unseen by the training/validation procedure used in model construction. The results showed that the size of the data is the deciding factor for the qualities of the generalization performance estimated from the validation set. We found that there was a significant gap between the performance estimated from the validation set and the one from the test set for the all the data splitting methods employed on small datasets. Such disparity decreased when more samples were available for training/validation, and this is because the models were then moving towards approximations of the central limit theory for the simulated datasets used. We also found that having too many or too few samples in the training set had a negative effect on the estimated model performance, suggesting that it is necessary to have a good balance between the sizes of training set and validation set to have a reliable estimation of model performance. We also found that systematic sampling method such as K-S and SPXY generally had very poor estimation of the model performance, most likely due to the fact that they are designed to take the most representative samples first and thus left a rather poorly representative sample set for model performance estimation.},
 author = {Xu, Yun and Goodacre, Royston},
 year = {2018},
 title = {On Splitting Training and Validation Set: A Comparative Study of Cross-Validation, Bootstrap and Systematic Sampling for Estimating the Generalization Performance of Supervised Learning},
 pages = {249--262},
 volume = {2},
 number = {3},
 issn = {2096-241X},
 journal = {Journal of analysis and testing},
 doi = {10.1007/s41664-018-0068-2},
 file = {Xu, Goodacre 2018 - On Splitting Training and Validation:Attachments/Xu, Goodacre 2018 - On Splitting Training and Validation.pdf:application/pdf}
}


