% This file was created with Citavi 6.11.0.0

@misc{.17.02.2020,
 abstract = {Monk AI Documentation Hauptseite},
 year = {17.02.2020},
 title = {MonkAI-Docs},
 url = {https://li8bot.github.io/monkai/#/home},
 urldate = {16.01.2022}
}


@article{Kalehbasti.2021,
 abstract = {Pricing a rental property on Airbnb is a challenging task for the owner as it determines the number of customers for the place. On the other hand, customers have to evaluate an offered price with minimal knowledge of an optimal value for the property. This paper aims to develop a reliable price prediction model using machine learning, deep learning, and natural language processing techniques to aid both the property owners and the customers with price evaluation given minimal available information about the property. Features of the rentals, owner characteristics, and the customer reviews will comprise the predictors, and a range of methods from linear regression to tree-based models, support-vector regression (SVR), K-means Clustering (KMC), and neural networks (NNs) will be used for creating the prediction model.},
 author = {Kalehbasti, Pouya Rezazadeh and Nikolenko, Liubov and Rezaei, Hoormazd},
 year = {2021},
 title = {Airbnb Price Prediction Using Machine Learning and Sentiment Analysis},
 pages = {173--184},
 volume = {12844},
 number = {3},
 journal = {International Cross-Domain Conference for Machine Learning and Knowledge Extraction, pp. 173-184. Springer, Cham},
 file = {Kalehbasti, Nikolenko et al. 2021 - Airbnb Price Prediction Using Machine (2):Attachments/Kalehbasti, Nikolenko et al. 2021 - Airbnb Price Prediction Using Machine (2).pdf:application/pdf}
}


@book{Leibe.2016,
 year = {2016},
 title = {Computer vision - ECCV 2016: 14th European conference, Amsterdam, The Netherlands, October 11-14, 2016 : proceedings},
 address = {Cham},
 volume = {9905},
 publisher = {Springer},
 isbn = {978-3-319-46447-3},
 series = {Lecture Notes in Computer Science},
 editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
 doi = {10.1007/978-3-319-46448-0}
}


@book{Lewis.February2016,
 author = {Lewis, N. D.},
 year = {February 2016},
 title = {Deep learning made easy with R : a gentle introduction for data science},
 address = {Place of publication not identified},
 publisher = {AusCov},
 isbn = {9781519514219},
 file = {N.D Lewis - Deep Learning Made Easy with R{\_} A Gentle Introduction For Data Science-CreateSpace Independent Publishing Platform (2016):Attachments/N.D Lewis - Deep Learning Made Easy with R{\_} A Gentle Introduction For Data Science-CreateSpace Independent Publishing Platform (2016).pdf:application/pdf}
}


@misc{Lin.07.08.2017,
 abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
 author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
 date = {07.08.2017},
 title = {Focal Loss for Dense Object Detection},
 url = {https://arxiv.org/pdf/1708.02002}
}


@incollection{Liu.2016,
 author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
 title = {SSD: Single Shot MultiBox Detector},
 pages = {21--37},
 volume = {9905},
 publisher = {Springer},
 isbn = {978-3-319-46447-3},
 series = {Lecture Notes in Computer Science},
 editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
 booktitle = {Computer vision - ECCV 2016},
 year = {2016},
 address = {Cham},
 doi = {10.1007/978-3-319-46448-0{\textunderscore }2}
}


@misc{MachineCurve.2019,
 abstract = {Leaky ReLU can improve your traditional ReLU by resolving some problems of the latter. Learn how and why it works. Explanations and examples.},
 author = {MachineCurve},
 year = {2019},
 title = {Leaky ReLU: improving traditional ReLU -- MachineCurve},
 url = {https://www.machinecurve.com/index.php/2019/10/15/leaky-relu-improving-traditional-relu/},
 urldate = {02.02.2022}
}


@book{James.2021,
 abstract = {Preface -- 1 Introduction -- 2 Statistical Learning -- 3 Linear Regression -- 4 Classification -- 5 Resampling Methods -- 6 Linear Model Selection and Regularization -- 7 Moving Beyond Linearity -- 8 Tree-Based Methods -- 9 Support Vector Machines -- 10 Deep Learning -- 11 Survival Analysis and Censored Data -- 12 Unsupervised Learning -- 13 Multiple Testing -- Index.



An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, deep learning, survival analysis, multiple testing, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra. This Second Edition features new chapters on deep learning, survival analysis, and multiple testing, as well as expanded treatments of na{\"i}ve Bayes, generalized linear models, Bayesian additive regression trees, and matrix completion. R code has been updated throughout to ensure compatibility.},
 author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
 year = {2021},
 title = {An Introduction to Statistical Learning: With Applications in R},
 address = {New York, NY},
 edition = {2nd ed. 2021},
 publisher = {{Springer US} and {Imprint: Springer}},
 isbn = {9781071614181},
 series = {Springer eBook Collection},
 doi = {10.1007/978-1-0716-1418-1},
 file = {ISLRv2{\_}website:Attachments/ISLRv2{\_}website.pdf:application/pdf}
}


@misc{MachineCurve.2019b,
 abstract = {Leaky ReLU can improve your traditional ReLU by resolving some problems of the latter. Learn how and why it works. Explanations and examples.},
 author = {MachineCurve},
 year = {2019},
 title = {Leaky ReLU: improving traditional ReLU -- MachineCurve},
 url = {https://www.machinecurve.com/index.php/2019/10/15/leaky-relu-improving-traditional-relu/},
 urldate = {02.02.2022}
}


@misc{NikhithTayambhath.2017,
 abstract = {I tried this code:

import cv2

image = cv2.imread({\textquotedbl}sample.jpg{\textquotedbl})

pixel = image[200, 550]

print pixel

But I am getting error as:

'Nonetype' no attributes error getitem

This error is getting displayed after executing the third line of code.},
 author = {{Nikhith Tayambhath}},
 year = {2017},
 title = {How to find the average colour of an image in Python with OpenCV?},
 url = {https://stackoverflow.com/questions/43111029/how-to-find-the-average-colour-of-an-image-in-python-with-opencv},
 urldate = {01.02.2022}
}


@misc{OpenGenusIQ:ComputingExpertise&Legacy.2020,
 abstract = {Beschreibung von Resnet50, default model in unserem object detector},
 author = {{OpenGenus IQ: Computing Expertise {\&} Legacy}},
 year = {2020},
 title = {Understanding ResNet50 architecture},
 url = {https://iq.opengenus.org/resnet50-architecture/},
 urldate = {16.01.2022}
}


@article{Poursaeed.2018,
 author = {Poursaeed, Omid and Matera, Tom{\'a}{\v{s}} and Belongie, Serge},
 year = {2018},
 title = {Vision-based real estate price estimation},
 pages = {667--676},
 volume = {29},
 number = {4},
 issn = {0932-8092},
 journal = {Machine Vision and Applications},
 doi = {10.1007/s00138-018-0922-2}
}


@misc{Redmon.08.06.2015,
 abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
 author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
 date = {08.06.2015},
 title = {You Only Look Once: Unified, Real-Time Object Detection},
 url = {https://arxiv.org/pdf/1506.02640},
 file = {1506.02640:Attachments/1506.02640.pdf:application/pdf}
}


@article{Simon.2011,
 abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of $\ell$1 and $\ell$2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
 author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
 year = {2011},
 title = {Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent},
 pages = {1--13},
 volume = {39},
 number = {5},
 issn = {1548-7660},
 journal = {Journal of statistical software},
 doi = {10.18637/jss.v039.i05},
 file = {Simon, Friedman et al. 2011 - Regularization Paths for Cox's Proportional:Attachments/Simon, Friedman et al. 2011 - Regularization Paths for Cox's Proportional.pdf:application/pdf}
}


@misc{StackOverflow.12.01.2022,
 author = {{Stack Overflow}},
 year = {12.01.2022},
 title = {r - Insert column at beginning of a data frame - Stack Overflow},
 url = {https://stackoverflow.com/questions/22178993/insert-column-at-beginning-of-a-data-frame?noredirect=1&lq=1},
 urldate = {12.01.2022},
 file = {Stack Overflow 12.01.2022 - r:Attachments/Stack Overflow 12.01.2022 - r.pdf:application/pdf}
}


@misc{NikhithTayambhath.2017b,
 abstract = {I tried this code:

import cv2

image = cv2.imread({\textquotedbl}sample.jpg{\textquotedbl})

pixel = image[200, 550]

print pixel

But I am getting error as:

'Nonetype' no attributes error getitem

This error is getting displayed after executing the third line of code.},
 author = {{Nikhith Tayambhath}},
 year = {2017},
 title = {How to find the average colour of an image in Python with OpenCV?},
 url = {https://stackoverflow.com/questions/43111029/how-to-find-the-average-colour-of-an-image-in-python-with-opencv},
 urldate = {29.01.2022}
}


@proceedings{Teredesai.2019,
 year = {2019},
 title = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery {\&} Data Mining},
 address = {New York,NY,United States},
 publisher = {{Association for Computing Machinery}},
 isbn = {9781450362016},
 series = {ACM Digital Library},
 editor = {Teredesai, Ankur},
 institution = {{ACM Special Interest Group on Management of Data} and {ACM Special Interest Group on Knowledge Discovery in Data}},
 doi = {10.1145/3292500}
}


@article{Hofner.2014,
 author = {Hofner, Benjamin and Mayr, Andreas and Robinzonov, Nikolay and Schmid, Matthias},
 year = {2014},
 title = {Model-based Boosting in R: A Hands-on Tutorial Using the  R Package mboost},
 pages = {3--35},
 volume = {29},
 journal = {Computational Statistics},
 file = {mboost:Attachments/mboost.pdf:application/pdf}
}


@book{Hastie.2009,
 author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
 year = {2009},
 title = {The elements of statistical learning: Data mining, inference, and prediction /   Trevor Hastie, Robert Tibshirani, Jerome Friedman},
 price = {{\pounds}55.99},
 address = {New York},
 edition = {2nd ed.},
 publisher = {Springer},
 isbn = {9780387848570},
 series = {Springer series in statistics},
 file = {ESLII:Attachments/ESLII.pdf:application/pdf}
}


@misc{.29.01.2021,
 year = {29.01.2021},
 title = {Colour Science for Python --- Colour 0.3.11 documentation},
 url = {https://colour.readthedocs.io/en/v0.3.11/index.html},
 urldate = {01.02.2022}
}


@misc{.29.01.2021b,
 year = {29.01.2021},
 title = {Colour Temperature --- Colour 0.3.15 documentation},
 url = {https://colour.readthedocs.io/en/v0.3.15/colour.temperature.html},
 urldate = {18.01.2022}
}


@misc{.04.10.2021,
 abstract = {Selenium um {\"u}ber Websiten zu crawlen und bilder herunter zu laden},
 year = {04.10.2021},
 title = {Selenium with Python --- Selenium Python Bindings 2 documentation},
 url = {https://selenium-python.readthedocs.io/},
 urldate = {16.01.2022}
}


@misc{.17.12.2021,
 abstract = {einige Informtionen {\"u}ber google colab},
 year = {17.12.2021},
 title = {Google Colab},
 url = {https://research.google.com/colaboratory/faq.html#resource-limits},
 urldate = {16.01.2022}
}


@misc{.16.01.2022,
 year = {16.01.2022},
 title = {Effect of batch size on training dynamics | by Kevin Shen | Mini Distill | Medium},
 url = {https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e},
 urldate = {16.01.2022}
}


@misc{.16.01.2022b,
 abstract = {Zusammenfassung von Monk AI funktionen},
 year = {16.01.2022},
 title = {Monk{\_}object{\_}detection},
 url = {https://awesomeopensource.com/project/Tessellate-Imaging/Monk_Object_Detection},
 urldate = {16.01.2022}
}


@misc{He.20.03.2017,
 abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
 author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
 date = {20.03.2017},
 title = {Mask R-CNN},
 url = {https://arxiv.org/pdf/1703.06870}
}


@misc{.16.01.2022c,
 abstract = {Beschreibung f{\"u}r ein Monk Obect{\_}Detection Projekt als Orientierung},
 year = {16.01.2022},
 title = {Object Detection made easy with Monk AI. | by Het Shah | The Startup | Medium},
 url = {https://medium.com/swlh/object-detection-made-easy-with-monk-ai-e994a0497b33},
 urldate = {16.01.2022}
}


@article{Bushaev.02.09.2018,
 abstract = {Disclaimer: I presume basic knowledge about neural network optimization algorithms. Particularly, knowledge about SGD and SGD with momentum will be very helpful to understand this post. RMSprop--- is$\ldots$},
 author = {Bushaev, Vitaly},
 year = {02.09.2018},
 title = {Understanding RMSprop --- faster neural network learning},
 url = {https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a},
 urldate = {02.02.2022},
 journal = {Towards Data Science},
 file = {Bushaev 02.09.2018 - Understanding RMSprop - faster neural:Attachments/Bushaev 02.09.2018 - Understanding RMSprop - faster neural.pdf:application/pdf}
}


@book{Chollet.2018,
 author = {Chollet, Fran{\c{c}}ois and Allaire, J. J.},
 year = {2018},
 title = {Deep learning with R},
 address = {Shelter Island},
 publisher = {{Manning Publications}},
 isbn = {9781617295546},
 file = {Fran{\c{c}}ois Chollet, J.J. Allaire - Deep Learning with R (2017, Manning Publications) - libgen.lc:Attachments/Fran{\c{c}}ois Chollet, J.J. Allaire - Deep Learning with R (2017, Manning Publications) - libgen.lc.pdf:application/pdf}
}


@misc{Dobovizki.18.01.2022,
 author = {Dobovizki, Nir},
 year = {18.01.2022},
 title = {Calculating the Perceived Brightness of a Color},
 url = {https://www.nbdtech.com/Blog/archive/2008/04/27/Calculating-the-Perceived-Brightness-of-a-Color.aspx},
 urldate = {18.01.2022}
}


@misc{GitHub.16.01.2022,
 abstract = {A one-stop repository for low-code easily-installable object detection pipelines. - Monk{\_}Object{\_}Detection/Example - Indoor Image Object Detection and Tagging.ipynb at master · Tessellate-Imaging/Monk{\_}Object{\_}Detection},
 author = {GitHub},
 year = {16.01.2022},
 title = {Monk{\_}Object{\_}Detection/Example - Indoor Image Object Detection and Tagging.ipynb at master · Tessellate-Imaging/Monk{\_}Object{\_}Detection},
 url = {https://github.com/Tessellate-Imaging/Monk_Object_Detection/blob/master/application_model_zoo/Example%20-%20Indoor%20Image%20Object%20Detection%20and%20Tagging.ipynb},
 urldate = {16.01.2022}
}


@misc{GitHub.16.01.2022b,
 abstract = {A one-stop repository for low-code easily-installable object detection pipelines. - Tessellate-Imaging/Monk{\_}Object{\_}Detection:},
 author = {GitHub},
 year = {16.01.2022},
 title = {Tessellate-Imaging/Monk{\_}Object{\_}Detection: A one-stop repository for low-code easily-installable object detection pipelines},
 url = {https://github.com/Tessellate-Imaging/Monk_Object_Detection},
 urldate = {16.01.2022}
}


@inproceedings{Haldar.2019,
 author = {Haldar, Malay and Abdool, Mustafa and Ramanathan, Prashant and Xu, Tao and Yang, Shulin and Duan, Huizhong and Zhang, Qing and Barrow-Williams, Nick and Turnbull, Bradley C. and Collins, Brendan M. and Legrand, Thomas},
 title = {Applying Deep Learning to Airbnb Search},
 pages = {1927--1935},
 publisher = {{Association for Computing Machinery}},
 isbn = {9781450362016},
 series = {ACM Digital Library},
 editor = {Teredesai, Ankur},
 booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery {\&} Data Mining},
 year = {2019},
 address = {New York,NY,United States},
 doi = {10.1145/3292500.3330658},
 file = {Haldar, Abdool et al. 07252019 - Applying Deep Learning to Airbnb:Attachments/Haldar, Abdool et al. 07252019 - Applying Deep Learning to Airbnb.pdf:application/pdf}
}


@misc{.01.02.2022,
 year = {01.02.2022},
 title = {Jupyter Notebook Viewer},
 url = {https://nbviewer.org/github/colour-science/colour-notebooks/blob/master/notebooks/temperature/cct.ipynb},
 urldate = {01.02.2022}
}


@article{Xu.2018,
 abstract = {Model validation is the most important part of building a supervised model. For building a~model with good generalization performance one must have a sensible data splitting strategy, and this is crucial for model validation. In this study, we conducted a comparative study on various reported data splitting methods. The MixSim model was employed to generate nine simulated datasets with different probabilities of mis-classification and variable sample sizes. Then partial least squares for discriminant analysis and support vector machines for classification were applied to these datasets. Data splitting methods tested included variants of cross-validation, bootstrapping, bootstrapped Latin partition, Kennard-Stone algorithm (K-S) and sample~set partitioning based on joint X-Y distances algorithm (SPXY). These methods were employed to split the data into training and validation sets. The estimated generalization performances from the validation sets were then compared with the ones obtained from the blind test sets which were generated from the same distribution but were unseen by the training/validation procedure used in model construction. The results showed that the size of the data is the deciding factor for the qualities of the generalization performance estimated from the validation set. We found that there was a significant gap between the performance estimated from the validation set and the one from the test set for the all the data splitting methods employed on small datasets. Such disparity decreased when more samples were available for training/validation, and this is because the models were then moving towards approximations of the central limit theory for the simulated datasets used. We also found that having too many or too few samples in the training set had a negative effect on the estimated model performance, suggesting that it is necessary to have a good balance between the sizes of training set and validation set to have a reliable estimation of model performance. We also found that systematic sampling method such as K-S and SPXY generally had very poor estimation of the model performance, most likely due to the fact that they are designed to take the most representative samples first and thus left a rather poorly representative sample set for model performance estimation.},
 author = {Xu, Yun and Goodacre, Royston},
 year = {2018},
 title = {On Splitting Training and Validation Set: A Comparative Study of Cross-Validation, Bootstrap and Systematic Sampling for Estimating the Generalization Performance of Supervised Learning},
 pages = {249--262},
 volume = {2},
 number = {3},
 issn = {2096-241X},
 journal = {Journal of analysis and testing},
 doi = {10.1007/s41664-018-0068-2},
 file = {Xu, Goodacre 2018 - On Splitting Training and Validation:Attachments/Xu, Goodacre 2018 - On Splitting Training and Validation.pdf:application/pdf}
}


