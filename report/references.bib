% This file was created with Citavi 6.11.0.0

@book{.2013,
 year = {2013},
 title = {Empirical Inference},
 publisher = {{Springer, Berlin, Heidelberg}}
}


@article{LeCUN.1995,
 abstract = {Convolutional networks for images, speech, and time series LE CUN Y. The handbook of brain theory and neural networks, 255-258, 1995},
 author = {{Le CUN}, Y.},
 year = {1995},
 title = {Convolutional networks for images, speech, and time series},
 url = {https://ci.nii.ac.jp/naid/10012743767/},
 pages = {255--258},
 journal = {The handbook of brain theory and neural networks}
}


@book{Leibe.2016,
 year = {2016},
 title = {Computer vision - ECCV 2016: 14th European conference, Amsterdam, The Netherlands, October 11-14, 2016 : proceedings},
 address = {Cham},
 volume = {9905},
 publisher = {Springer},
 isbn = {978-3-319-46447-3},
 series = {Lecture Notes in Computer Science},
 editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
 doi = {10.1007/978-3-319-46448-0}
}


@book{Lewis.February2016,
 author = {Lewis, N. D.},
 year = {February 2016},
 title = {Deep learning made easy with R : a gentle introduction for data science},
 publisher = {{Manning Publications}},
 isbn = {9781519514219},
 file = {N.D Lewis - Deep Learning Made Easy with R{\_} A Gentle Introduction For Data Science-CreateSpace Independent Publishing Platform (2016):Attachments/N.D Lewis - Deep Learning Made Easy with R{\_} A Gentle Introduction For Data Science-CreateSpace Independent Publishing Platform (2016).pdf:application/pdf}
}


@misc{Lin.09.12.2016,
 abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
 author = {Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
 date = {09.12.2016},
 title = {Feature Pyramid Networks for Object Detection},
 url = {https://arxiv.org/pdf/1612.03144}
}


@misc{Lin.07.08.2017,
 abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
 author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
 date = {07.08.2017},
 title = {Focal Loss for Dense Object Detection},
 url = {https://arxiv.org/pdf/1708.02002}
}


@incollection{Liu.2016,
 author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
 title = {SSD: Single Shot MultiBox Detector},
 pages = {21--37},
 volume = {9905},
 publisher = {Springer},
 isbn = {978-3-319-46447-3},
 series = {Lecture Notes in Computer Science},
 editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
 booktitle = {Computer vision - ECCV 2016},
 year = {2016},
 address = {Cham},
 doi = {10.1007/978-3-319-46448-0{\textunderscore }2}
}


@misc{MachineCurve.2019,
 abstract = {Leaky ReLU can improve your traditional ReLU by resolving some problems of the latter. Learn how and why it works. Explanations and examples.},
 author = {MachineCurve},
 year = {2019},
 title = {Leaky ReLU: improving traditional ReLU -- MachineCurve},
 url = {https://www.machinecurve.com/index.php/2019/10/15/leaky-relu-improving-traditional-relu/},
 urldate = {02.02.2022}
}


@book{MartinRiedmiller.1992,
 abstract = {CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): In this paper, a new learning algorithm, RPROP, is proposed. To overcome the inherent disadvantages of the pure gradient-descent technique of the original backpropagation procedure, RPROP performs an adaptation of the weight update-values according to the behaviour of the errorfunction. The results of RPROP on several learning tasks are shown in comparison to other well-known adaptive learning algorithms. 1 Introduction  Backpropagation is the most widely used algorithm for supervised learning with multilayered feed-forward networks. The basic idea of the backpropagation learning algorithm is the repeated application of the chain rule to compute the influence of each weight in the network with respect to an arbitrary errorfunction E [1]: @E @w ij  =  @E @a i  @a i  @net i  @net i  @w ij  (1) where w ij is the weight from neuron j to neuron i, a i is the activation value and net i  is the weighted sum of the inputs of neuron i. Once the partial derivative for each weight is known, the a...},
 author = {{Martin Riedmiller}, Heinrich Braun},
 year = {1992},
 title = {RPROP - A Fast Adaptive Learning Algorithm},
 url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.4576}
}


@book{Masters.2018,
 abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput.

In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size $m$.

The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between {\$}m = 2{\$} and {\$}m = 32{\$}, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
 author = {Masters, Dominic and Luschi, Carlo},
 year = {2018},
 title = {Revisiting Small Batch Training for Deep Neural Networks},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1804.07612},
 file = {Masters, Luschi 2018 - Revisiting Small Batch Training:Attachments/Masters, Luschi 2018 - Revisiting Small Batch Training.pdf:application/pdf}
}


@book{Montavon.2012,
 year = {2012},
 title = {Neural Networks: Tricks of the Trade},
 address = {Berlin, Heidelberg},
 volume = {7700},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-642-35288-1},
 series = {Lecture Notes in Computer Science},
 editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
 doi = {10.1007/978-3-642-35289-8}
}


@misc{NikhithTayambhath.2017,
 abstract = {I tried this code:

import cv2

image = cv2.imread({\textquotedbl}sample.jpg{\textquotedbl})

pixel = image[200, 550]

print pixel

But I am getting error as:

'Nonetype' no attributes error getitem

This error is getting displayed after executing the third line of code.},
 author = {{Nikhith Tayambhath}},
 year = {2017},
 title = {How to find the average colour of an image in Python with OpenCV?},
 url = {https://stackoverflow.com/questions/43111029/how-to-find-the-average-colour-of-an-image-in-python-with-opencv},
 urldate = {01.02.2022}
}


@misc{OpenGenusIQ:ComputingExpertise&Legacy.2020,
 abstract = {Beschreibung von Resnet50, default model in unserem object detector},
 author = {{OpenGenus IQ: Computing Expertise {\&} Legacy}},
 year = {2020},
 title = {Understanding ResNet50 architecture},
 url = {https://iq.opengenus.org/resnet50-architecture/},
 urldate = {16.01.2022}
}


@incollection{PavanKumar.2020,
 author = {{Pavan Kumar}, Illa and {Hara Gopal}, V. P. and Ramasubbareddy, Somula and Nalluri, Sravani and Govinda, K.},
 title = {Dominant Color Palette Extraction by K-Means Clustering Algorithm and Reconstruction of Image},
 pages = {921--929},
 volume = {1079},
 publisher = {{Springer Singapore}},
 isbn = {978-981-15-1096-0},
 series = {Advances in Intelligent Systems and Computing},
 editor = {Raju, K. Srujan and Senkerik, Roman and Lanka, Satya Prasad and Rajagopal, V.},
 booktitle = {Data Engineering and Communication Technology},
 year = {2020},
 address = {Singapore},
 doi = {10.1007/978-981-15-1097-7{\textunderscore }78}
}


@article{Kalehbasti.2021,
 abstract = {Pricing a rental property on Airbnb is a challenging task for the owner as it determines the number of customers for the place. On the other hand, customers have to evaluate an offered price with minimal knowledge of an optimal value for the property. This paper aims to develop a reliable price prediction model using machine learning, deep learning, and natural language processing techniques to aid both the property owners and the customers with price evaluation given minimal available information about the property. Features of the rentals, owner characteristics, and the customer reviews will comprise the predictors, and a range of methods from linear regression to tree-based models, support-vector regression (SVR), K-means Clustering (KMC), and neural networks (NNs) will be used for creating the prediction model.},
 author = {Kalehbasti, Pouya Rezazadeh and Nikolenko, Liubov and Rezaei, Hoormazd},
 year = {2021},
 title = {Airbnb Price Prediction Using Machine Learning and Sentiment Analysis},
 pages = {173--184},
 volume = {12844},
 number = {3},
 journal = {International Cross-Domain Conference for Machine Learning and Knowledge Extraction, pp. 173-184. Springer, Cham},
 file = {Kalehbasti, Nikolenko et al. 2021 - Airbnb Price Prediction Using Machine (2):Attachments/Kalehbasti, Nikolenko et al. 2021 - Airbnb Price Prediction Using Machine (2).pdf:application/pdf}
}


@article{Poursaeed.2018,
 author = {Poursaeed, Omid and Matera, Tom{\'a}{\v{s}} and Belongie, Serge},
 year = {2018},
 title = {Vision-based real estate price estimation},
 pages = {667--676},
 volume = {29},
 number = {4},
 issn = {0932-8092},
 journal = {Machine Vision and Applications},
 doi = {10.1007/s00138-018-0922-2}
}


@inproceedings{Redmon.2016,
 author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
 title = {You Only Look Once: Unified, Real-Time Object Detection},
 url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html},
 pages = {779--788},
 year = {2016},
 file = {Redmon, Divvala et al. 2016 - You Only Look Once:Attachments/Redmon, Divvala et al. 2016 - You Only Look Once.pdf:application/pdf}
}


@misc{ResearchGate.15.02.2022,
 abstract = {Download scientific diagram | The architecture of ResNet-50 model. from publication: Performance Evaluation of Deep CNN-Based Crack Detection and Localization Techniques for Concrete Structures | This paper proposes a customized convolutional neural network for crack detection in concrete structures. The proposed method is compared to four existing deep learning methods based on training data size, data heterogeneity, network complexity, and the number of epochs. The... | Concrete Structures, Performance Evaluation and Localization | ResearchGate, the professional network for scientists.},
 author = {ResearchGate},
 year = {15.02.2022},
 title = {Figure 6. The architecture of ResNet-50 model},
 url = {https://www.researchgate.net/figure/The-architecture-of-ResNet-50-model_fig4_349717475},
 urldate = {15.02.2022}
}


@incollection{Schapire.2013,
 abstract = {BoostingBoosting---( is an approach to machine learning based on the idea of creating a highly accurate prediction rule by combining many relatively weak and inaccurate rules. The AdaBoost algorithm of...},
 author = {Schapire, Robert E.},
 title = {Explaining AdaBoost},
 url = {https://link.springer.com/chapter/10.1007/978-3-642-41136-6_5},
 pages = {37--52},
 publisher = {{Springer, Berlin, Heidelberg}},
 booktitle = {Empirical Inference},
 year = {2013},
 doi = {10.1007/978-3-642-41136-6{\textunderscore }5}
}


@article{Seif.21.05.2019,
 abstract = {A loss function in Machine Learning is a measure of how accurately your ML model is able to predict the expected outcome i.e the ground truth. The loss function will take two items as input: the$\ldots$},
 author = {Seif, George},
 year = {21.05.2019},
 title = {Understanding the 3 most common loss functions for Machine Learning Regression},
 url = {https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3},
 urldate = {04.03.2022},
 journal = {Towards Data Science}
}


@article{Sharma.06.09.2017,
 abstract = {As you can see the function is a line or linear. Therefore, the output of the functions will not be confined between any range. It doesn't help with the complexity or various parameters of usual data$\ldots$},
 author = {Sharma, Sagar},
 year = {06.09.2017},
 title = {Activation Functions in Neural Networks - Towards Data Science},
 url = {https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6},
 urldate = {28.02.2022},
 journal = {Towards Data Science}
}


@article{Sharma.06.09.2017b,
 abstract = {As you can see the function is a line or linear. Therefore, the output of the functions will not be confined between any range. It doesn't help with the complexity or various parameters of usual data$\ldots$},
 author = {Sharma, Sagar},
 year = {06.09.2017},
 title = {Activation Functions in Neural Networks - Towards Data Science},
 url = {https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6},
 urldate = {05.03.2022},
 journal = {Towards Data Science}
}


@article{Simon.2011,
 abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of $\ell$1 and $\ell$2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
 author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
 year = {2011},
 title = {Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent},
 pages = {1--13},
 volume = {39},
 number = {5},
 issn = {1548-7660},
 journal = {Journal of statistical software},
 doi = {10.18637/jss.v039.i05},
 file = {Simon, Friedman et al. 2011 - Regularization Paths for Cox's Proportional:Attachments/Simon, Friedman et al. 2011 - Regularization Paths for Cox's Proportional.pdf:application/pdf}
}


@article{Srivastava.2014,
 abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different {\textquotedbl}thinned{\textquotedbl} networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
 author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
 year = {2014},
 title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
 keywords = {deep learning;model combination;neural networks;regularization},
 pages = {1929--1958},
 volume = {15},
 number = {1},
 issn = {1532-4435},
 journal = {J. Mach. Learn. Res.}
}


@misc{StackOverflow.04.03.2022,
 author = {{Stack Overflow}},
 year = {04.03.2022},
 title = {python - Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 - Stack Overflow},
 url = {https://stackoverflow.com/questions/47068709/your-cpu-supports-instructions-that-this-tensorflow-binary-was-not-compiled-to-u},
 urldate = {04.03.2022}
}


@article{Stottner.16.05.2019,
 abstract = {Detailed explanation of why normalizing data helps neural networks and why tanh generally performs better than the logistic sigmoid as an activation function.},
 author = {St{\"o}ttner, Timo},
 year = {16.05.2019},
 title = {Why Data should be Normalized before Training a Neural Network},
 url = {https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d},
 urldate = {05.03.2022},
 journal = {Towards Data Science}
}


@proceedings{Teredesai.2019,
 year = {2019},
 title = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery {\&} Data Mining},
 address = {New York,NY,United States},
 publisher = {{Association for Computing Machinery}},
 isbn = {9781450362016},
 series = {ACM Digital Library},
 editor = {Teredesai, Ankur},
 institution = {{ACM Special Interest Group on Management of Data} and {ACM Special Interest Group on Knowledge Discovery in Data}},
 doi = {10.1145/3292500}
}


@misc{TessellateImaging.16.01.2022,
 abstract = {A one-stop repository for low-code easily-installable object detection pipelines. - Tessellate-Imaging/Monk{\_}Object{\_}Detection:},
 author = {{Tessellate Imaging}},
 year = {16.01.2022},
 title = {Tessellate-Imaging/Monk{\_}Object{\_}Detection},
 url = {https://github.com/Tessellate-Imaging/Monk_Object_Detection},
 urldate = {16.01.2022}
}


@misc{Thielmann.2021,
 author = {Thielmann, Anton},
 year = {2021},
 title = {Simple tutorial on exploratory model building on a subset of the Airbnb dataset},
 series = {Statistical and Deep Learning WS 21/22},
 institutions = {{University of G{\"o}ttingen}},
 file = {Data Set Introduction:Attachments/Data Set Introduction.ipynb:application/octet-stream}
}


@book{Raju.2020,
 year = {2020},
 title = {Data Engineering and Communication Technology},
 address = {Singapore},
 volume = {1079},
 publisher = {{Springer Singapore}},
 isbn = {978-981-15-1096-0},
 series = {Advances in Intelligent Systems and Computing},
 editor = {Raju, K. Srujan and Senkerik, Roman and Lanka, Satya Prasad and Rajagopal, V.},
 doi = {10.1007/978-981-15-1097-7}
}


@article{Xu.2018,
 abstract = {Model validation is the most important part of building a supervised model. For building a~model with good generalization performance one must have a sensible data splitting strategy, and this is crucial for model validation. In this study, we conducted a comparative study on various reported data splitting methods. The MixSim model was employed to generate nine simulated datasets with different probabilities of mis-classification and variable sample sizes. Then partial least squares for discriminant analysis and support vector machines for classification were applied to these datasets. Data splitting methods tested included variants of cross-validation, bootstrapping, bootstrapped Latin partition, Kennard-Stone algorithm (K-S) and sample~set partitioning based on joint X-Y distances algorithm (SPXY). These methods were employed to split the data into training and validation sets. The estimated generalization performances from the validation sets were then compared with the ones obtained from the blind test sets which were generated from the same distribution but were unseen by the training/validation procedure used in model construction. The results showed that the size of the data is the deciding factor for the qualities of the generalization performance estimated from the validation set. We found that there was a significant gap between the performance estimated from the validation set and the one from the test set for the all the data splitting methods employed on small datasets. Such disparity decreased when more samples were available for training/validation, and this is because the models were then moving towards approximations of the central limit theory for the simulated datasets used. We also found that having too many or too few samples in the training set had a negative effect on the estimated model performance, suggesting that it is necessary to have a good balance between the sizes of training set and validation set to have a reliable estimation of model performance. We also found that systematic sampling method such as K-S and SPXY generally had very poor estimation of the model performance, most likely due to the fact that they are designed to take the most representative samples first and thus left a rather poorly representative sample set for model performance estimation.},
 author = {Xu, Yun and Goodacre, Royston},
 year = {2018},
 title = {On Splitting Training and Validation Set: A Comparative Study of Cross-Validation, Bootstrap and Systematic Sampling for Estimating the Generalization Performance of Supervised Learning},
 pages = {249--262},
 volume = {2},
 number = {3},
 issn = {2096-241X},
 journal = {Journal of analysis and testing},
 doi = {10.1007/s41664-018-0068-2},
 file = {Xu, Goodacre 2018 - On Splitting Training and Validation:Attachments/Xu, Goodacre 2018 - On Splitting Training and Validation.pdf:application/pdf}
}


@book{James.2021,
 abstract = {Preface -- 1 Introduction -- 2 Statistical Learning -- 3 Linear Regression -- 4 Classification -- 5 Resampling Methods -- 6 Linear Model Selection and Regularization -- 7 Moving Beyond Linearity -- 8 Tree-Based Methods -- 9 Support Vector Machines -- 10 Deep Learning -- 11 Survival Analysis and Censored Data -- 12 Unsupervised Learning -- 13 Multiple Testing -- Index.



An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, deep learning, survival analysis, multiple testing, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra. This Second Edition features new chapters on deep learning, survival analysis, and multiple testing, as well as expanded treatments of na{\"i}ve Bayes, generalized linear models, Bayesian additive regression trees, and matrix completion. R code has been updated throughout to ensure compatibility.},
 author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
 year = {2021},
 title = {An Introduction to Statistical Learning: With Applications in R},
 address = {New York, NY},
 edition = {2nd ed. 2021},
 publisher = {{Springer US} and {Imprint: Springer}},
 isbn = {9781071614181},
 series = {Springer eBook Collection},
 doi = {10.1007/978-1-0716-1418-1},
 file = {ISLRv2{\_}website:Attachments/ISLRv2{\_}website.pdf:application/pdf}
}


@article{Hofner.2014,
 author = {Hofner, Benjamin and Mayr, Andreas and Robinzonov, Nikolay and Schmid, Matthias},
 year = {2014},
 title = {Model-based Boosting in R: A Hands-on Tutorial Using the  R Package mboost},
 pages = {3--35},
 volume = {29},
 journal = {Computational Statistics},
 file = {mboost:Attachments/mboost.pdf:application/pdf}
}


@proceedings{.2016,
 year = {2016}
}


@proceedings{.2016b,
 year = {2016},
 title = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 publisher = {IEEE}
}


@proceedings{.2017,
 year = {2017},
 title = {2017 IEEE International Conference on Computer Vision (ICCV)},
 publisher = {IEEE}
}


@misc{.01.11.2019,
 year = {01.11.2019},
 title = {HSP Color Model - Alternative to HSV (HSB) and HSL},
 url = {http://alienryderflex.com/hsp.html},
 urldate = {19.02.2022}
}


@article{AguirrePablo.2017,
 abstract = {We demonstrate the viability of using four low-cost smartphone cameras to perform Tomographic PIV. We use colored shadows to imprint two or three different time-steps on the same image. The back-lighting is accomplished with three sets of differently-colored pulsed LEDs. Each set of Red, Green {\&} Blue LEDs is shone on a diffuser screen facing each of the cameras. We thereby record the RGB-colored shadows of opaque suspended particles, rather than the conventionally used scattered light. We subsequently separate the RGB color channels, to represent the separate times, with preprocessing to minimize noise and cross-talk. We use commercially available Tomo-PIV software for the calibration, 3-D particle reconstruction and particle-field correlations, to obtain all three velocity components in a volume. Acceleration estimations can be done thanks to the triple pulse illumination. Our test flow is a vortex ring produced by forcing flow through a circular orifice, using a flexible membrane, which is driven by a pressurized air pulse. Our system is compared to a commercial stereoscopic PIV system for error estimations. We believe this proof of concept experiment will make this technique available for education, industry and scientists for a fraction of the hardware cost needed for traditional Tomo-PIV.},
 author = {Aguirre-Pablo, Andres A. and Alarfaj, Meshal K. and Li, Er Qiang and Hern{\'a}ndez-S{\'a}nchez, J. F. and Thoroddsen, Sigurdur T.},
 year = {2017},
 title = {Tomographic Particle Image Velocimetry using Smartphones and Colored Shadows},
 pages = {3714},
 volume = {7},
 number = {1},
 journal = {Scientific reports},
 doi = {10.1038/s41598-017-03722-9}
}


@misc{Airbnb.02.03.2022,
 abstract = {Entdecke, was neu auf Airbnb ist.},
 author = {Airbnb},
 year = {02.03.2022},
 title = {Airbnb: einzigartige Unterk{\"u}nfte und Aktivit{\"a}ten},
 url = {https://www.airbnb.ch/},
 urldate = {02.03.2022}
}


@article{alAli.2021,
 abstract = {This paper proposes a customized convolutional neural network for crack detection in concrete structures. The proposed method is compared to four existing deep learning methods based on training data size, data heterogeneity, network complexity, and the number of epochs. The performance of the proposed convolutional neural network (CNN) model is evaluated and compared to pretrained networks, i.e., the VGG-16, VGG-19, ResNet-50, and Inception V3 models, on eight datasets of different sizes, created from two public datasets. For each model, the evaluation considered computational time, crack localization results, and classification measures, e.g., accuracy, precision, recall, and F1-score. Experimental results demonstrated that training data size and heterogeneity among data samples significantly affect model performance. All models demonstrated promising performance on a limited number of diverse training data; however, increasing the training data size and reducing diversity reduced generalization performance, and led to overfitting. The proposed customized CNN and VGG-16 models outperformed the other methods in terms of classification, localization, and computational time on a small amount of data, and the results indicate that these two models demonstrate superior crack detection and localization for concrete structures.},
 author = {alAli, Luqman and Alnajjar, Fady and Jassmi, Hamad Al and Gocho, Munkhjargal and Khan, Wasif and Serhani, M. Adel},
 year = {2021},
 title = {Performance Evaluation of Deep CNN-Based Crack Detection and Localization Techniques for Concrete Structures},
 volume = {21},
 number = {5},
 journal = {Sensors (Basel, Switzerland)},
 doi = {10.3390/s21051688},
 file = {Ali, Alnajjar et al. 2021 - Performance Evaluation of Deep CNN-Based:Attachments/Ali, Alnajjar et al. 2021 - Performance Evaluation of Deep CNN-Based.pdf:application/pdf}
}


@misc{Ames.25.02.2022,
 abstract = {How do you choose lighting color temperature? This can be tricky, but we have a guide for choosing CCT and CRI as you look to design lighting for a space.},
 author = {Ames, Jeremy},
 year = {25.02.2022},
 title = {What is CCT? A guide to choosing correlated color temperature for your lighting},
 url = {https://insights.regencylighting.com/what-is-correlated-color-temperature-cct-and-how-do-you-choose-it-for-your-lighting},
 urldate = {27.02.2022}
}


@incollection{Bengio.2012,
 author = {Bengio, Yoshua},
 title = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
 pages = {437--478},
 volume = {7700},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-642-35288-1},
 series = {Lecture Notes in Computer Science},
 editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
 booktitle = {Neural Networks: Tricks of the Trade},
 year = {2012},
 address = {Berlin, Heidelberg},
 doi = {10.1007/978-3-642-35289-8{\textunderscore }26},
 file = {Bengio 2012 - Practical Recommendations for Gradient-Based Training:Attachments/Bengio 2012 - Practical Recommendations for Gradient-Based Training.pdf:application/pdf}
}


@article{Breiman.2001,
 abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
 author = {Breiman, Leo},
 year = {2001},
 title = {Random Forests},
 url = {https://link.springer.com/article/10.1023/a:1010933404324},
 pages = {5--32},
 volume = {45},
 number = {1},
 issn = {1573-0565},
 journal = {Machine Learning},
 doi = {10.1023/A:1010933404324},
 file = {Breiman 2001 - Random Forests:Attachments/Breiman 2001 - Random Forests.pdf:application/pdf}
}


@book{Breiman.2017,
 abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
 author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
 year = {2017},
 title = {Classification And Regression Trees},
 url = {https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman-jerome-friedman-richard-olshen-charles-stone},
 publisher = {Routledge},
 isbn = {9781315139470},
 doi = {10.1201/9781315139470},
 file = {Breiman, Friedman et al. 2017 - Classification And Regression Trees:Attachments/Breiman, Friedman et al. 2017 - Classification And Regression Trees.pdf:application/pdf}
}


@article{Bushaev.02.09.2018,
 abstract = {Disclaimer: I presume basic knowledge about neural network optimization algorithms. Particularly, knowledge about SGD and SGD with momentum will be very helpful to understand this post. RMSprop--- is$\ldots$},
 author = {Bushaev, Vitaly},
 year = {02.09.2018},
 title = {Understanding RMSprop --- faster neural network learning},
 url = {https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a},
 urldate = {02.02.2022},
 journal = {Towards Data Science},
 file = {Bushaev 02.09.2018 - Understanding RMSprop - faster neural:Attachments/Bushaev 02.09.2018 - Understanding RMSprop - faster neural.pdf:application/pdf}
}


@book{Chollet.2018,
 author = {Chollet, Fran{\c{c}}ois and Allaire, J. J.},
 year = {2018},
 title = {Deep learning with R},
 address = {Shelter Island},
 publisher = {{Manning Publications}},
 isbn = {9781617295546},
 file = {Fran{\c{c}}ois Chollet, J.J. Allaire - Deep Learning with R (2017, Manning Publications) - libgen.lc:Attachments/Fran{\c{c}}ois Chollet, J.J. Allaire - Deep Learning with R (2017, Manning Publications) - libgen.lc.pdf:application/pdf}
}


@misc{InsideAirbnb.2022,
 abstract = {Inside Airbnb is an independent, non-commercial set of tools and data that allows you to explore how Airbnb is REALLY being used in cities around the world.},
 author = {{Inside Airbnb}},
 year = {2022},
 title = {Inside Airbnb. Adding data to the debate},
 url = {http://insideairbnb.com/about.html},
 urldate = {01.03.2022}
}


@misc{DevelopPaper.2020,
 abstract = {Provide a platform for developers to answer questions, learn and exchange programming knowledge, and create an era that belongs to developers!},
 author = {{Develop Paper}},
 year = {2020},
 title = {Activation function of attention mechanism: adaptive parameterized relu activation function - Develop Paper},
 url = {https://developpaper.com/activation-function-of-attention-mechanism-adaptive-parameterized-relu-activation-function/},
 urldate = {19.02.2022}
}


@article{Dong.2013,
 abstract = {The impact of missing data on quantitative research can be serious, leading to biased estimates of parameters, loss of information, decreased statistical power, increased standard errors, and weakened generalizability of findings. In this paper, we discussed and demonstrated three principled missing data methods: multiple imputation, full information maximum likelihood, and expectation-maximization algorithm, applied to a real-world data set. Results were contrasted with those obtained from the complete data set and from the listwise deletion method. The relative merits of each method are noted, along with common features they share. The paper concludes with an emphasis on the importance of statistical assumptions, and recommendations for researchers. Quality of research will be enhanced if (a) researchers explicitly acknowledge missing data problems and the conditions under which they occurred, (b) principled methods are employed to handle missing data, and (c) the appropriate treatment of missing data is incorporated into review standards of manuscripts submitted for publication.},
 author = {Dong, Yiran and Peng, Chao-Ying Joanne},
 year = {2013},
 title = {Principled missing data methods for researchers},
 pages = {222},
 volume = {2},
 number = {1},
 issn = {2193-1801},
 journal = {SpringerPlus},
 doi = {10.1186/2193-1801-2-222},
 file = {Dong, Peng 2013 - Principled missing data methods:Attachments/Dong, Peng 2013 - Principled missing data methods.pdf:application/pdf}
}


@book{Efron.1993,
 author = {Efron, Bradley and Tibshirani, Robert J.},
 year = {1993},
 title = {An introduction to the bootstrap},
 address = {New York},
 publisher = {{Chapman {\&} Hall}},
 isbn = {978-0-412-04231-7},
 doi = {10.1007/978-1-4899-4541-9},
 file = {Efron, Tibshirani 1993 - An introduction to the bootstrap:Attachments/Efron, Tibshirani 1993 - An introduction to the bootstrap.pdf:application/pdf}
}


@article{Friedman.2010,
 abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include $\ell$(1) (the lasso), $\ell$(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
 author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
 year = {2010},
 title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
 pages = {1--22},
 volume = {33},
 number = {1},
 issn = {1548-7660},
 journal = {Journal of statistical software}
}


@misc{Girshick.11.11.2013,
 abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
 author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
 date = {11.11.2013},
 title = {Rich feature hierarchies for accurate object detection and semantic  segmentation},
 url = {https://arxiv.org/pdf/1311.2524},
 file = {Girshick, Donahue et al. 11.11.2013 - Rich feature hierarchies for accurate:Attachments/Girshick, Donahue et al. 11.11.2013 - Rich feature hierarchies for accurate.pdf:application/pdf}
}


@misc{GitHub.16.01.2022,
 abstract = {A one-stop repository for low-code easily-installable object detection pipelines. - Monk{\_}Object{\_}Detection/Example - Indoor Image Object Detection and Tagging.ipynb at master · Tessellate-Imaging/Monk{\_}Object{\_}Detection},
 author = {GitHub},
 year = {16.01.2022},
 title = {Monk{\_}Object{\_}Detection/Example - Indoor Image Object Detection and Tagging.ipynb at master · Tessellate-Imaging/Monk{\_}Object{\_}Detection},
 url = {https://github.com/Tessellate-Imaging/Monk_Object_Detection/blob/master/application_model_zoo/Example%20-%20Indoor%20Image%20Object%20Detection%20and%20Tagging.ipynb},
 urldate = {16.01.2022}
}


@article{Gneiting.2007,
 abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G ? F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
 author = {Gneiting, Tilmann and Raftery, Adrian E.},
 year = {2007},
 title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
 pages = {359--378},
 volume = {102},
 number = {477},
 issn = {0162-1459},
 journal = {Journal of the American Statistical Association},
 doi = {10.1198/016214506000001437},
 file = {Gneiting, Raftery 2007 - Strictly Proper Scoring Rules:Attachments/Gneiting, Raftery 2007 - Strictly Proper Scoring Rules.pdf:application/pdf}
}


@inproceedings{Haldar.2019,
 author = {Haldar, Malay and Abdool, Mustafa and Ramanathan, Prashant and Xu, Tao and Yang, Shulin and Duan, Huizhong and Zhang, Qing and Barrow-Williams, Nick and Turnbull, Bradley C. and Collins, Brendan M. and Legrand, Thomas},
 title = {Applying Deep Learning to Airbnb Search},
 pages = {1927--1935},
 publisher = {{Association for Computing Machinery}},
 isbn = {9781450362016},
 series = {ACM Digital Library},
 editor = {Teredesai, Ankur},
 booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery {\&} Data Mining},
 year = {2019},
 address = {New York,NY,United States},
 doi = {10.1145/3292500.3330658},
 file = {Haldar, Abdool et al. 07252019 - Applying Deep Learning to Airbnb:Attachments/Haldar, Abdool et al. 07252019 - Applying Deep Learning to Airbnb.pdf:application/pdf}
}


@book{Hastie.2009,
 author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
 year = {2009},
 title = {The elements of statistical learning: Data mining, inference, and prediction /   Trevor Hastie, Robert Tibshirani, Jerome Friedman},
 price = {{\pounds}55.99},
 address = {New York},
 edition = {2nd ed.},
 publisher = {Springer},
 isbn = {9780387848570},
 series = {Springer series in statistics},
 file = {ESLII:Attachments/ESLII.pdf:application/pdf}
}


@inproceedings{He.2017,
 author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
 title = {Mask R-CNN},
 publisher = {IEEE},
 booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
 year = {2017},
 doi = {10.1109/iccv.2017.322}
}


@misc{He.10.12.2015,
 abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
 author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
 date = {10.12.2015},
 title = {Deep Residual Learning for Image Recognition},
 url = {https://arxiv.org/pdf/1512.03385},
 file = {He, Zhang et al. 10.12.2015 - Deep Residual Learning for Image:Attachments/He, Zhang et al. 10.12.2015 - Deep Residual Learning for Image.pdf:application/pdf}
}


@inproceedings{He.2016,
 author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
 title = {Deep Residual Learning for Image Recognition},
 publisher = {IEEE},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2016},
 doi = {10.1109/cvpr.2016.90},
 file = {He, Zhang et al. 2016 - Deep Residual Learning for Image:Attachments/He, Zhang et al. 2016 - Deep Residual Learning for Image.pdf:application/pdf}
}


@article{HernandezAndres.1999,
 abstract = {Natural outdoor illumination daily undergoes large changes in its correlated color temperature (CCT), yet existing equations for calculating CCT from chromaticity coordinates span only part of this range. To improve both the gamut and accuracy of these CCT calculations, we use chromaticities calculated from our measurements of nearly 7000 daylight and skylight spectra to test an equation that accurately maps CIE 1931 chromaticities x and y into CCT. We extend the work of McCamy [Color Res. Appl. 12, 285-287 (1992)] by using a chromaticity epicenter for CCT and the inverse slope of the line that connects it to x and y. With two epicenters for different CCT ranges, our simple equation is accurate across wide chromaticity and CCT ranges (3000-10(6) K) spanned by daylight and skylight.},
 author = {Hern{\'a}ndez-Andr{\'e}s, J. and Lee, R. L. and Romero, J.},
 year = {1999},
 title = {Calculating correlated color temperatures across the entire gamut of daylight and skylight chromaticities},
 pages = {5703--5709},
 volume = {38},
 number = {27},
 issn = {1559-128X},
 journal = {Applied optics},
 doi = {10.1364/ao.38.005703}
}


@misc{Hinton.03.07.2012,
 abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This {\textquotedbl}overfitting{\textquotedbl} is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random {\textquotedbl}dropout{\textquotedbl} gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
 author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
 date = {03.07.2012},
 title = {Improving neural networks by preventing co-adaptation of feature  detectors},
 url = {https://arxiv.org/pdf/1207.0580}
}


@misc{Dobovizki.18.01.2022,
 author = {Dobovizki, Nir},
 year = {18.01.2022},
 title = {Calculating the Perceived Brightness of a Color},
 url = {https://www.nbdtech.com/Blog/archive/2008/04/27/Calculating-the-Perceived-Brightness-of-a-Color.aspx},
 urldate = {18.01.2022}
}


@misc{Zeng.2018,
 abstract = {Introduction Recently I have been doing some research on object detection, trying to find a state-of-the-art detector for a project. I found several popular detectors including: OverFeat (Sermanet et al. 2013), R-CNN (Girshick et al. 2013), Fast R-CNN (Girshick 2015), SSD (Liu et al. 2016), R-FCN (Dai et al. 2016), YOLO (Redmon et al. 2016), Faster R-CNN (Ren et al. 2017) and RetinaNet (Lin, Goyal, et al. 2017). According to the paper, RetinaNet showed both ideal accuracy and speed compared to other detectors while still keeping a very simple construct; plus, there is an opensource implementaion by Gaiser et al.},
 author = {Zeng, Nick},
 year = {2018},
 title = {RetinaNet Explained and Demystified | NickZeng|曾广宇},
 url = {https://blog.zenggyu.com/en/post/2018-12-05/retinanet-explained-and-demystified/#fn2},
 urldate = {26.02.2022}
}


